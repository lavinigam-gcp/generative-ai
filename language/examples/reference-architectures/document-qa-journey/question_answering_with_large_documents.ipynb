{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Question Answering with Large Documents - Foundations\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/examples/reference-architectures/question_answering_with_large_documents.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/reference-architectures/question_answering_with_large_documents.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/reference-architectures/question_answering_with_large_documents.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "You are a Data Scientist in a company and have been tasked to build a question-answering system using Vertex PaLM API. The system should be able to take the company's documents and extract or query important information. You are simply building an out-of-box question-answering bot on enterprise data that can be in any format, such as \"PDF,\" \"DOC,\" \"TXT,\" \"DOCX,\" \"PPTX,\" \"HTML,\" etc.  \n",
    "\n",
    "The token limit is the current challenge and limitation with large language models, including [Vertex PaLM API](). As you have seen in the basic [question-answering notebook](), to get the best results from PaLM 2 Model, you must provide relevant context while asking `closed-domain` questions. Unfortunately, enterprise data do not follow such restrictions, and underlying documents could have thousands or millions of pages, which means you won't be able to pass them as context. \n",
    "\n",
    "In this `reference-architecture`, you will see two methods that can address the large context challenge, known as : \n",
    "\n",
    "* Chunk-based Q&A - splitting documents in smaller chunks.\n",
    "* Chunk-Based Embeddeding Q&A - creating embeddings of smaller chunks and using vector similarity search to find relevant context. \n",
    "\n",
    "The notebook introduces you to the foundational theory of handling huge documents for building a question-answering bot using Vertex PaLM API and finding relevant context for a user query, keeping the context limitation in check. \n",
    "\n",
    "In addition, there can be open source or Google Cloud drop-in replacement of steps, which will be discussed later in the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "By the end of the notebook, you will learn how to build a question-answering system using PaLM API, which can handle large documents. \n",
    "You will also learn the conceptual implementation of two methods to help you embed large contexts from many documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDU0XJ1xRDlL"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a5AEr0lkLKD"
   },
   "source": [
    "### Install Vertex AI SDK & Other dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tesseract-ocr is already the newest version (4.0.0-2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 9 not upgraded.\n",
      "libtesseract-dev is already the newest version (4.0.0-2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 9 not upgraded.\n",
      "E: Package 'pstotext' has no installation candidate\n"
     ]
    }
   ],
   "source": [
    "#Base system dependencies\n",
    "!sudo apt -y -qq install tesseract-ocr\n",
    "!sudo apt -y -qq install libtesseract-dev\n",
    "!sudo apt-get -y -qq install poppler-utils #required by PyPDF2 for page count and other pdf utilities\n",
    "!sudo apt-get -y -qq install python-dev libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig\n",
    "#Python dependencies\n",
    "!pip install google-cloud-aiplatform --upgrade --quiet --user\n",
    "!pip install pytesseract --quiet --user\n",
    "!pip install PyPDF2 --quiet --user\n",
    "!pip install textract --quiet --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Colab only**: Uncomment the following cell to restart the kernel. For Vertex AI Workbench you can restart the terminal using the button on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_Hsqwn4hkLKE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xe7OuYuGkLKF"
   },
   "source": [
    "### Authenticating your notebook environment\n",
    "* If you are using **Colab** to run this notebook, uncomment the cell below and continue.\n",
    "* If you are using **Vertex AI Workbench**, check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9Gx2SAZkLKF"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vertexai.preview.language_models import TextGenerationModel,TextEmbeddingModel\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from PyPDF2 import PdfReader\n",
    "import glob\n",
    "import textract\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UP76a2la7O-a"
   },
   "source": [
    "### Import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7isig7e07O-a"
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "embedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make PaLM API calls more resilient since you will make many calls to the API in this notebook, you should allow specific API calls to be retried before they fail. We should not \"hammer\" or \"congest\" our underlying foundation models, so it is wise to wait a bit before a retry. \n",
    "\n",
    "You can add a simple delay before every request; however, adding a fixed delay only helps if it allows the service enough time to catch up. Alternatively, a simple approach is to increase the length of the delay for each subsequent attempt. \n",
    "\n",
    "The constant used for the delay may need to be adjusted depending on the service you're calling. This is commonly refered to as [exponential backoff](https://en.wikipedia.org/wiki/Exponential_backoff)\n",
    "\n",
    "You can find more retry strategies that works best for Google Cloud APIs [here](https://cloud.google.com/storage/docs/retry-strategy) and API guide for the current method [here](https://tenacity.readthedocs.io/en/latest/api.html)\n",
    "\n",
    "You can see an example below to make a function that implements a \"retry and exponential backoff.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To avoid hitting quota limit since you might hit the APIs multiple times in this notebook. \n",
    "# Each retry occurs at a random time in a geometrically expanding interval. \n",
    "# It allows for a custom multiplier and an ability to restrict the upper limit of the random interval to some maximum value.\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n",
    "def text_generation_model_with_backoff(**kwargs):\n",
    "    return generation_model.predict(**kwargs).text\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n",
    "def embedding_model_with_backoff(text=[]):\n",
    "    embeddings = embedding_model.get_embeddings(text)\n",
    "    return [each.values for each in embeddings][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIPcn5dZ7O-b"
   },
   "source": [
    "## Question Answering with large Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Method\n",
    "Before you dive deeper into possible methods for large document question-answering, explore the primary process of building the system and how it fails with larger files and context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Basic Method](https://storage.googleapis.com/document-examples-llm/assets/basic_method_flow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Loader\n",
    "You start by loading the documents from your source. In this case, we are dumping them from Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://document-examples-llm/documents/20230426_alphabet_10Q.pdf...\n",
      "Copying gs://document-examples-llm/documents/20210203_alphabet_10K.pdf...       \n",
      "Copying gs://document-examples-llm/documents/practitioners_guide_to_mlops_whitepaper.docx...\n",
      "Copying gs://document-examples-llm/documents/20230203_alphabet_10K.pdf...       \n",
      "Copying gs://document-examples-llm/documents/practitioners_guide_to_mlops_whitepaper.pdf...\n",
      "Copying gs://document-examples-llm/documents/MLOps Whitepaper.json...           \n",
      "Copying gs://document-examples-llm/documents/MLOps Whitepaper.pptx...\n",
      "Copying gs://document-examples-llm/documents/practitioners_guide_to_mlops_whitepaper.txt...\n",
      "Copying gs://document-examples-llm/documents/20220202_alphabet_10K.pdf...       \n",
      "Copying gs://document-examples-llm/documents/mlops_whitepaper_page18.jpg...\n",
      "Copying gs://document-examples-llm/documents/mlops_whitepaper_page22.png...     \n",
      "/ [11/12 files][ 17.6 MiB/ 17.6 MiB]  99% Done                                  \r"
     ]
    }
   ],
   "source": [
    "#Copying the files from the GCS bucket to local \n",
    "!gsutil -m cp -r gs://document-examples-llm/documents ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Processor\n",
    "\n",
    "Once you have the documents, it's time to process them. In the processor phase, your goal is to read the documents and convert them into the required format to be easily used in the downstream logic. While reading, you should maintain as much metadata as possible of the source document. \n",
    "\n",
    "Here, you can observe that we are loading different file types like PDF, TXT, DOCX, and JSON. Each file type has its reader, and you can use a simple open-source library called [textract](https://textract.readthedocs.io/en/stable/) and [PyPDF2](https://pypdf2.readthedocs.io/en/3.0.0/) to load them. For this data, you can save `file-name`, `file_type`, `page_number` (only for pdf), and `content` for each file. \n",
    "\n",
    "This metadata will be essential to quote the source of information while sending it as a context and answering queries at later stage. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.9 s, sys: 60.9 ms, total: 24 s\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "final_data = []\n",
    "for name in glob.glob('documents/*'):\n",
    "    file_type = name.split(\".\")[-1]\n",
    "    if file_type == \"pdf\":\n",
    "        reader = PdfReader(name)\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                data_packet = {}\n",
    "                data_packet['file_name'] = name.split(\"/\")[-1]\n",
    "                data_packet['file_type'] = \"pdf\"\n",
    "                data_packet['page_number'] = int(i+1)\n",
    "                data_packet['content'] = text\n",
    "                final_data.append(data_packet) \n",
    "    if file_type == \"txt\":\n",
    "        text_txt = textract.process(name).decode(\"utf-8\")\n",
    "        data_packet = {}\n",
    "        data_packet['file_name'] = name.split(\"/\")[-1]\n",
    "        data_packet['file_type'] = \"txt\"\n",
    "        data_packet['page_number'] = None\n",
    "        data_packet['content'] = text_txt\n",
    "        final_data.append(data_packet)\n",
    "    if file_type == \"docx\":\n",
    "        text_docx = textract.process(name).decode(\"utf-8\")\n",
    "        data_packet = {}\n",
    "        data_packet['file_name'] = name.split(\"/\")[-1]\n",
    "        data_packet['file_type'] = \"docx\"\n",
    "        data_packet['page_number'] = None\n",
    "        data_packet['content'] = text_docx\n",
    "        final_data.append(data_packet)\n",
    "    if file_type == \"json\":\n",
    "        text_json = textract.process(name).decode(\"utf-8\")\n",
    "        data_packet = {}\n",
    "        data_packet['file_name'] = name.split(\"/\")[-1]\n",
    "        data_packet['file_type'] = \"json\"\n",
    "        data_packet['page_number'] = None\n",
    "        data_packet['content'] = text_json\n",
    "        final_data.append(data_packet) \n",
    "        \n",
    "        \n",
    "    # Add readers and processors for other file types [\"doc\",\"pptx\",\"jpeg\" etc.] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a simple `document store` is important once you have read the documents with all the necessary metadata. Think of document store as a tabular representation of all the files and their content with essential metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_type</th>\n",
       "      <th>page_number</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>UNITED STATES\\nSECURITIES AND EXCHANGE COMMISS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Large accelerated filer ☒  Accelerated filer ☐...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Alphabet Inc.\\nForm 10-K\\nFor the Fiscal Year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NOTE ABOUT FORWARD-LOOKING STATEMENTS\\nThis An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>5.0</td>\n",
       "      <td>•the sufficiency and timing of our proposed re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33\\nML metadata tracking is generally integrat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34\\nFigure 14 shows the tasks that are involve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35\\nFigure 15.  End-to-end MLOps workflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36\\nAdditional resources\\nFor more information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>practitioners_guide_to_mlops_whitepaper.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White paper\\nMay 2021\\n\\nPractitioners guide t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>372 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       file_name file_type  page_number  \\\n",
       "0                      20210203_alphabet_10K.pdf       pdf          1.0   \n",
       "1                      20210203_alphabet_10K.pdf       pdf          2.0   \n",
       "2                      20210203_alphabet_10K.pdf       pdf          3.0   \n",
       "3                      20210203_alphabet_10K.pdf       pdf          4.0   \n",
       "4                      20210203_alphabet_10K.pdf       pdf          5.0   \n",
       "..                                           ...       ...          ...   \n",
       "367  practitioners_guide_to_mlops_whitepaper.pdf       pdf         33.0   \n",
       "368  practitioners_guide_to_mlops_whitepaper.pdf       pdf         34.0   \n",
       "369  practitioners_guide_to_mlops_whitepaper.pdf       pdf         35.0   \n",
       "370  practitioners_guide_to_mlops_whitepaper.pdf       pdf         36.0   \n",
       "371  practitioners_guide_to_mlops_whitepaper.txt       txt          NaN   \n",
       "\n",
       "                                               content  \n",
       "0    UNITED STATES\\nSECURITIES AND EXCHANGE COMMISS...  \n",
       "1    Large accelerated filer ☒  Accelerated filer ☐...  \n",
       "2    Alphabet Inc.\\nForm 10-K\\nFor the Fiscal Year ...  \n",
       "3    NOTE ABOUT FORWARD-LOOKING STATEMENTS\\nThis An...  \n",
       "4    •the sufficiency and timing of our proposed re...  \n",
       "..                                                 ...  \n",
       "367  33\\nML metadata tracking is generally integrat...  \n",
       "368  34\\nFigure 14 shows the tasks that are involve...  \n",
       "369          35\\nFigure 15.  End-to-end MLOps workflow  \n",
       "370  36\\nAdditional resources\\nFor more information...  \n",
       "371  White paper\\nMay 2021\\n\\nPractitioners guide t...  \n",
       "\n",
       "[372 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting the data that has been read from GCS to Pandas DataFrame for easy readibility and downstream logic \n",
    "pdf_data = pd.DataFrame.from_dict(final_data)\n",
    "pdf_data = pdf_data.sort_values(by=['file_name','page_number']) #sorting the datafram by filename and page_number\n",
    "pdf_data.reset_index(inplace=True,drop=True)\n",
    "pdf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document store has these different file types : \n",
      " pdf     369\n",
      "json      1\n",
      "docx      1\n",
      "txt       1\n",
      "Name: file_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# you can check how many different file type we have in our document store. \n",
    "print(\"Document store has these different file types : \\n\", pdf_data['file_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the next step in the conventional method is to pass the context to PaLM API while asking the question. \n",
    "\n",
    "You don't know which document will be helpful, so we you can go ahead and use all the document's text present in `content` column as context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total words in the context:  1531642\n"
     ]
    }
   ],
   "source": [
    "# combining all the content of the PDF as single string such that it can be passed as context.\n",
    "context = '\\n'.join(str(v) for v in pdf_data['content'].values)\n",
    "print(\"The total words in the context: \",len(context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can write a simple prompt along with the question. Then, you can preempt the prompt by making it follow some basic instructions. In the prompt, you only ask to answer if it finds the answer in the given `context`. \n",
    "\n",
    "You are dynamically passing the context and the question so that you can change it as per requirements and experimentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is the address for google headquarter\"\n",
    "prompt = f\"\"\"Answer the question as precise as possible using the provided context. If the answer is\n",
    "              not contained in the context, say \"answer not available in context\" \\n\\n\n",
    "            Context: \\n {context}?\\n\n",
    "            Question: \\n {question} \\n\n",
    "            Answer:\n",
    "          \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vertex PaLM API - Answer Extraction & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's the moment of truth. In your prompt, you are passing so many words as context (roughly all documents). \n",
    "\n",
    "You already know that we have a input \n",
    "(prompt) token limit of [8192 tokens](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models) for the `text-bison@001` model, so your PaLM API call should fail. Because, as per ~8k token limit, the PaLM model is expecting ~6k words (input token). However, we are sending  ~ `1531642` words just as a prompt. \n",
    "\n",
    "As a reminder, a single token may be smaller than a word. A token is approximately four characters. Therefore, 100 tokens correspond to roughly 60-80 words. \n",
    "\n",
    "Hence, you know why conventional methods would not work when you want to do question-answering on large documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code failed since it won't be able to run inference on such a huge context and throws this exception:  400 Request contains an invalid argument.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"PaLM Predicted:\",generation_model.predict(\n",
    "        prompt\n",
    "    ).text)\n",
    "except Exception as e: \n",
    "    print(\"The code failed since it won't be able to run inference on such a huge context and throws this exception: \",e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you can still run the code, if you restrict the context to first 1000 words or something which is lesser than the token limit for PaLM API. But there is a good chance you will miss getting the expected answer, since your context might be missing in the first 1000 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1290\n",
      "PaLM Predicted: answer not available in context\n"
     ]
    }
   ],
   "source": [
    "question = \"How much google invested in waymo?\"\n",
    "prompt = f\"\"\"Answer the question as precise as possible using the provided context. If the answer is\n",
    "              not contained in the context, say \"answer not available in context\" \\n\\n\n",
    "            Context: \\n {context[:1000]}?\\n\n",
    "            Question: \\n {question} \\n\n",
    "            Answer:\n",
    "          \"\"\"\n",
    "print(len(prompt))\n",
    "print(\"PaLM Predicted:\",generation_model.predict(\n",
    "    prompt\n",
    ").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now you have seen how stuffing the whole document content of so many files is not a very promising method to build question-answering systems. There are many different methods to address this limitation, but as discussed in the overview section, you will see two foundational and important methods: \n",
    "\n",
    "* Chunk-based Q&A\n",
    "* Chunk-Based Embeddeding Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Chunk-based Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Chunk-based Q&A, rather than passing the whole document as context, we break the document into manageable chunks and then design a prompt to ask each chunk the question. \n",
    "\n",
    "The chunks should be configured considering the input token limit (roughly 8k). Sometimes, it would be dividing the documents page-wise or splitting the pages into smaller paragraphs. It ultimately depends on the documents you have at your source. \n",
    "\n",
    "However, one thing remains constant: the chunk size should be defined by input token limit (roughly 8k), which would mean roughly not more than 6k (approximate) words.\n",
    "\n",
    "The typical flow for this method goes like this:\n",
    "\n",
    "* You take N documents from your source. \n",
    "* Split documents into N chunks (let's say 1000 words for each chunk) \n",
    "* Each chunk should be passed as context to the question-answer prompt\n",
    "* Summarize the answers from each chunk either as a `column` or persist that into the persistence layer. \n",
    "\n",
    "\n",
    "You can refer to the below digram for more clarity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![chunk learning](https://storage.googleapis.com/document-examples-llm/assets/chunklearning_flow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start by writing a simple function `get_chunks_iter` that takes a long string `s` and the size of the chunk as `maxlength`. \n",
    "\n",
    "This function aims to divide input string `s` into the size of `maxlength` - which are total words in that chunk and, save all the individual chunks into a list, and return `final_chunk` list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_iter(s, maxlength):\n",
    "    start = 0\n",
    "    end = 0\n",
    "    final_chunk = []\n",
    "    while start + maxlength  < len(s) and end != -1:\n",
    "        end = s.rfind(\" \", start, start + maxlength + 1)\n",
    "        final_chunk.append(s[start:end])\n",
    "        start = end +1\n",
    "    final_chunk.append(s[start:])\n",
    "    return final_chunk\n",
    "\n",
    "#function to apply \"get_chunks_iter\" function on each row of document store. \n",
    "def split_text(row):\n",
    "    chunk_iter =  get_chunks_iter(row, chunk_size)\n",
    "    return chunk_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_type</th>\n",
       "      <th>page_number</th>\n",
       "      <th>content</th>\n",
       "      <th>chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>UNITED STATES SECURITIES AND EXCHANGE COMMISSI...</td>\n",
       "      <td>UNITED STATES SECURITIES AND EXCHANGE COMMISSI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>UNITED STATES SECURITIES AND EXCHANGE COMMISSI...</td>\n",
       "      <td>Nasdaq Stock Market LLC Nasdaq Global Select M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>UNITED STATES SECURITIES AND EXCHANGE COMMISSI...</td>\n",
       "      <td>registrant was required to submit such files Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Large accelerated filer Accelerated filer Non ...</td>\n",
       "      <td>Large accelerated filer Accelerated filer Non ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Large accelerated filer Accelerated filer Non ...</td>\n",
       "      <td>2020 was approximately 849 7 billion For purpo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   file_name file_type  page_number  \\\n",
       "0  20210203_alphabet_10K.pdf       pdf          1.0   \n",
       "1  20210203_alphabet_10K.pdf       pdf          1.0   \n",
       "2  20210203_alphabet_10K.pdf       pdf          1.0   \n",
       "3  20210203_alphabet_10K.pdf       pdf          2.0   \n",
       "4  20210203_alphabet_10K.pdf       pdf          2.0   \n",
       "\n",
       "                                             content  \\\n",
       "0  UNITED STATES SECURITIES AND EXCHANGE COMMISSI...   \n",
       "1  UNITED STATES SECURITIES AND EXCHANGE COMMISSI...   \n",
       "2  UNITED STATES SECURITIES AND EXCHANGE COMMISSI...   \n",
       "3  Large accelerated filer Accelerated filer Non ...   \n",
       "4  Large accelerated filer Accelerated filer Non ...   \n",
       "\n",
       "                                              chunks  \n",
       "0  UNITED STATES SECURITIES AND EXCHANGE COMMISSI...  \n",
       "1  Nasdaq Stock Market LLC Nasdaq Global Select M...  \n",
       "2  registrant was required to submit such files Y...  \n",
       "3  Large accelerated filer Accelerated filer Non ...  \n",
       "4  2020 was approximately 849 7 billion For purpo...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global chunk_size\n",
    "# you can define how many words should be there in a given chunk. \n",
    "chunk_size = 1000\n",
    "\n",
    "pdf_data_sample = pdf_data.copy()\n",
    "# Remove all non-alphabets and numbers from the data to clean it up. \n",
    "# This is harsh cleaning. You can define your custom logic for cleansing here. \n",
    "pdf_data_sample['content'] = pdf_data_sample['content'].apply(lambda x: re.sub('[^A-Za-z0-9]+', ' ', x))\n",
    "# Apply the chunk splitting logic here on each row of content in document store. \n",
    "pdf_data_sample['chunks'] = pdf_data_sample['content'].apply(split_text)\n",
    "# Now, each row in 'chunks' contains list of all chunks and hence we need to explode them into individual rows. \n",
    "pdf_data_sample = pdf_data_sample.explode(\"chunks\")\n",
    "# Sort and reset index \n",
    "pdf_data_sample = pdf_data_sample.sort_values(by=['file_name','page_number'])\n",
    "pdf_data_sample.reset_index(inplace=True,drop=True)\n",
    "pdf_data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe how a single page in the `20210203_alphabet_10K.pdf` file is divided into three chunks. \n",
    "\n",
    "You have three pages with the same \"1\" indicating that a page has been divided into three subsets (chunks). This is important because now you have a manageable chunk to send as context, rather than whole document as seen before.\n",
    "\n",
    "This will increase the total number of rows in the document store as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original document store has : 372  rows without chunking\n",
      "The chunked document store has : 1604  rows with chunking\n"
     ]
    }
   ],
   "source": [
    "print(\"The original document store has :\",pdf_data.shape[0],\" rows without chunking\") \n",
    "print(\"The chunked document store has :\",pdf_data_sample.shape[0],\" rows with chunking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can define two prompts: \n",
    "\n",
    "1) For asking a direct question with a similar strategy as seen before with the conventional method. However, pass the `chunks` column this time since you should only pass a single chunk for each PaLM API call on each row. \n",
    "\n",
    "2) For extracting specific entities like dates, amounts, product names, etc. This prompt will be helpful if you have any particular entities that must be extracted from the documents. This prompt doesn't require any specific question to be asked. In fact, you can make the prompt one-shot or multi-shot prompt by providing examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pass in the apply function on document store to extract answer for specific question on each row. \n",
    "def get_answer(df):\n",
    "    prompt = f\"\"\"Answer the question as precise as possible using the provided context. If the answer is\n",
    "                not contained in the context, say \"answer not available in context\" \\n\\n\n",
    "              Context: \\n {df['chunks']}?\\n\n",
    "              Question: \\n {question} \\n\n",
    "              Answer:\n",
    "            \"\"\"\n",
    "\n",
    "    pred =  text_generation_model_with_backoff(\n",
    "      prompt=prompt\n",
    "    )\n",
    "    return pred\n",
    "\n",
    "#function to pass in the apply function on document store to extract dates from each row. \n",
    "def get_dates(df):\n",
    "    prompt = f\"\"\"Extract dates from the given context along with significance of those dates. If the dates are not available then\n",
    "              say \"dates not available\". Do not mention any date which is not given in the context \\n\\n\n",
    "              context: \\n\n",
    "              {df['chunks']}\n",
    "            \"\"\"\n",
    "\n",
    "    pred =  text_generation_model_with_backoff(\n",
    "      prompt=prompt\n",
    "    )\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_type</th>\n",
       "      <th>page_number</th>\n",
       "      <th>content</th>\n",
       "      <th>chunks</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>predicted_dates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>UNITED STATES SECURITIES AND EXCHANGE COMMISSI...</td>\n",
       "      <td>UNITED STATES SECURITIES AND EXCHANGE COMMISSI...</td>\n",
       "      <td>1600 Amphitheatre Parkway Mountain View CA 94043</td>\n",
       "      <td>The fiscal year end date is December 31, 2020.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>UNITED STATES SECURITIES AND EXCHANGE COMMISSI...</td>\n",
       "      <td>Nasdaq Stock Market LLC Nasdaq Global Select M...</td>\n",
       "      <td>answer not available in context</td>\n",
       "      <td>Dates not available.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>UNITED STATES SECURITIES AND EXCHANGE COMMISSI...</td>\n",
       "      <td>registrant was required to submit such files Y...</td>\n",
       "      <td>answer not available in context</td>\n",
       "      <td>dates not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Large accelerated filer Accelerated filer Non ...</td>\n",
       "      <td>Large accelerated filer Accelerated filer Non ...</td>\n",
       "      <td>answer not available in context</td>\n",
       "      <td>June 30 2020: aggregate market value of shares...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Large accelerated filer Accelerated filer Non ...</td>\n",
       "      <td>2020 was approximately 849 7 billion For purpo...</td>\n",
       "      <td>answer not available in context</td>\n",
       "      <td>The date 2020 is mentioned in the context.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   file_name file_type  page_number  \\\n",
       "0  20210203_alphabet_10K.pdf       pdf          1.0   \n",
       "1  20210203_alphabet_10K.pdf       pdf          1.0   \n",
       "2  20210203_alphabet_10K.pdf       pdf          1.0   \n",
       "3  20210203_alphabet_10K.pdf       pdf          2.0   \n",
       "4  20210203_alphabet_10K.pdf       pdf          2.0   \n",
       "\n",
       "                                             content  \\\n",
       "0  UNITED STATES SECURITIES AND EXCHANGE COMMISSI...   \n",
       "1  UNITED STATES SECURITIES AND EXCHANGE COMMISSI...   \n",
       "2  UNITED STATES SECURITIES AND EXCHANGE COMMISSI...   \n",
       "3  Large accelerated filer Accelerated filer Non ...   \n",
       "4  Large accelerated filer Accelerated filer Non ...   \n",
       "\n",
       "                                              chunks  \\\n",
       "0  UNITED STATES SECURITIES AND EXCHANGE COMMISSI...   \n",
       "1  Nasdaq Stock Market LLC Nasdaq Global Select M...   \n",
       "2  registrant was required to submit such files Y...   \n",
       "3  Large accelerated filer Accelerated filer Non ...   \n",
       "4  2020 was approximately 849 7 billion For purpo...   \n",
       "\n",
       "                                   predicted_answer  \\\n",
       "0  1600 Amphitheatre Parkway Mountain View CA 94043   \n",
       "1                   answer not available in context   \n",
       "2                   answer not available in context   \n",
       "3                   answer not available in context   \n",
       "4                   answer not available in context   \n",
       "\n",
       "                                     predicted_dates  \n",
       "0     The fiscal year end date is December 31, 2020.  \n",
       "1                               Dates not available.  \n",
       "2                                dates not available  \n",
       "3  June 30 2020: aggregate market value of shares...  \n",
       "4         The date 2020 is mentioned in the context.  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can take a small sample of the whole document store to avoid making too many calls to the API. \n",
    "pdf_data_sample_head = pdf_data_sample.head(20)\n",
    "\n",
    "question = \"what is the address for google headquarter\"\n",
    "pdf_data_sample_head['predicted_answer'] = pdf_data_sample_head.apply(get_answer,axis=1)\n",
    "pdf_data_sample_head['predicted_dates'] = pdf_data_sample_head.apply(get_dates,axis=1)\n",
    "pdf_data_sample_head.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now look into the results to see how our PaLM performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Context]\n",
      "UNITED STATES SECURITIES AND EXCHANGE COMMISSION Washington D C 20549 FORM 10 K Mark One ANNUAL REPORT PURSUANT TO SECTION 13 OR 15 d OF THE SECURITIES EXCHANGE ACT OF 1934 For the fiscal year ende d December 31 2020 OR TRANSITION REPORT PURSUANT TO SECTION 13 OR 15 d OF THE SECURITIES EXCHANGE ACT OF 1934 For the transition period from to Commission file number 001 37580 Alphabet Inc Exact name of registrant as specified in its charter Delaware 61 1767919 State or other jurisdiction of incorporation or organization I R S Employer Identification No 1600 Amphitheatre Parkway Mountain View CA 94043 Address of principal executive offices including zip code 650 253 000 0 Registrant s telephone number including area code Securities registered pursuant to Section 12 b of the Act Title of each class Trading Symbol s Name of each exchange on which registered Class A Common Stock 0 001 par value GOOGL Nasdaq Stock Market LLC Nasdaq Global Select Market Class C Capital Stock 0 001 par value GOOG\n",
      "\n",
      "\n",
      " [Extracted Answer]\n",
      "1600 Amphitheatre Parkway Mountain View CA 94043\n",
      "\n",
      "\n",
      " [Source: page_number]\n",
      "1.0\n",
      "\n",
      "\n",
      " [Source: file_name]\n",
      "20210203_alphabet_10K.pdf\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"[Context]\")\n",
    "print(pdf_data_sample_head['chunks'].iloc[index])\n",
    "print(\"\\n\\n [Extracted Answer]\")\n",
    "print(pdf_data_sample_head['predicted_answer'].iloc[index])\n",
    "print(\"\\n\\n [Source: page_number]\")\n",
    "print(pdf_data_sample_head['page_number'].iloc[index])\n",
    "print(\"\\n\\n [Source: file_name]\")\n",
    "print(pdf_data_sample_head['file_name'].iloc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Context]\n",
      "UNITED STATES SECURITIES AND EXCHANGE COMMISSION Washington D C 20549 FORM 10 K Mark One ANNUAL REPORT PURSUANT TO SECTION 13 OR 15 d OF THE SECURITIES EXCHANGE ACT OF 1934 For the fiscal year ende d December 31 2020 OR TRANSITION REPORT PURSUANT TO SECTION 13 OR 15 d OF THE SECURITIES EXCHANGE ACT OF 1934 For the transition period from to Commission file number 001 37580 Alphabet Inc Exact name of registrant as specified in its charter Delaware 61 1767919 State or other jurisdiction of incorporation or organization I R S Employer Identification No 1600 Amphitheatre Parkway Mountain View CA 94043 Address of principal executive offices including zip code 650 253 000 0 Registrant s telephone number including area code Securities registered pursuant to Section 12 b of the Act Title of each class Trading Symbol s Name of each exchange on which registered Class A Common Stock 0 001 par value GOOGL Nasdaq Stock Market LLC Nasdaq Global Select Market Class C Capital Stock 0 001 par value GOOG\n",
      "\n",
      "\n",
      " [Extracted Dates]\n",
      "The fiscal year end date is December 31, 2020.\n",
      "\n",
      "\n",
      " [Source: page_number]\n",
      "1.0\n",
      "\n",
      "\n",
      " [Source: file_name]\n",
      "20210203_alphabet_10K.pdf\n"
     ]
    }
   ],
   "source": [
    "print(\"[Context]\")\n",
    "print(pdf_data_sample_head['chunks'].iloc[index])\n",
    "print(\"\\n\\n [Extracted Dates]\")\n",
    "print(pdf_data_sample_head['predicted_dates'].iloc[index])\n",
    "print(\"\\n\\n [Source: page_number]\")\n",
    "print(pdf_data_sample_head['page_number'].iloc[index])\n",
    "print(\"\\n\\n [Source: file_name]\")\n",
    "print(pdf_data_sample_head['file_name'].iloc[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look into this method's various pros and cons to summarize what you have done. \n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* Precision: The precision of the answers increases using this method since the context is exact because of chunking.\n",
    "* Entity Extraction: This is the most helpful method if your answer is available across different document levels. If there is a specific number or entity you are looking to extract available on most pages, then this is the most efficient and precise method. \n",
    "\n",
    "**Cons:**\n",
    "\n",
    "*   Multiple API Call: Each subset/chunk will call the API. It might incur costs and may take time. You can also hit the API limit quotas if the number of documents is enormous. \n",
    "*   Slow: If your answers are available at early chunks/subsets, even then, it will search through all the chunks/subsets. It's very similar to binary search. You can make it efficient by devising clever strategies to make sure it stops when it finds relevant information.\n",
    "* Conflicting Answers: What if the relevant answer is found in multiple chunks and subsets? How would you know the correct answer? You will likely find overlapping answers. You should devise some clever ranking algorithms to fix that.\n",
    "\n",
    "Moving forward, let's explore the following method, which addresses some of the shortcomings of Method 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Chunk-Based Embeddeding Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest cons you saw with the previous method for typical Q&A is that you would have to call PaLM API on all the chunks. As you have seen, it's suitable for precision and extraction tasks, but more efficient ways exist for direct question-answering. One such method is creating embeddings of the chunks. \n",
    "\n",
    "When you have embeddings (vector/number representations of text) of each chunk, you can use simple vector mathematics to find similar/dissimilar chunks across the document store. Why is that helpful?\n",
    "\n",
    "Rather than going through each chunk till N chunks (image N to be very large), you can find the relevant context from all the chunks in the document store where your answer may exist. \n",
    "\n",
    "Like, when you ask about \"Google headquarters\", you should have a way where only those chunks should be selected where there is some information about \"Google headquarters\" and not every chunk. This saves you considerable computation and API costs. \n",
    "\n",
    "The typical flow for this method goes like this: \n",
    "* You take N documents from your source\n",
    "* Split documents into N chunks (let's say 1000 words for each chunk)\n",
    "* Create embeddings for each chunk and store them in the document store. It can now be called vector DB or vector document storage. \n",
    "* Convert the question that you want to ask also to embeddings. \n",
    "* Now you have two sets of vectors - 1) chunk vectors and 2) query/question vector\n",
    "* You can perform a cosine similarity between these vectors to find all closed chunks.\n",
    "* Once you have the cosine scores, you can sort them, pick the top M and use this as a context for the prompt. \n",
    "* Finally, hit the PaLM API with only the relevant context. As you can observe, you didn't have to call the API on call chunks. \n",
    "\n",
    "You can refer to the below digram for more clarity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Embedding Learning](https://storage.googleapis.com/document-examples-llm/assets/embeddinglearning.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start the implementation first by simply getting the embeddings for each chunk. You are still working on the top 20 head data points to avoid any significant calls to the API. \n",
    "\n",
    "This will add the embeddings (vector/number representation) of each chunk as a separate column, and we can now call this a vector db or vector document store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_type</th>\n",
       "      <th>page_number</th>\n",
       "      <th>content</th>\n",
       "      <th>chunks</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>predicted_dates</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>UNITED STATES SECURITIES AND EXCHANGE COMMISSI...</td>\n",
       "      <td>UNITED STATES SECURITIES AND EXCHANGE COMMISSI...</td>\n",
       "      <td>1600 Amphitheatre Parkway Mountain View CA 94043</td>\n",
       "      <td>The fiscal year end date is December 31, 2020.</td>\n",
       "      <td>[-0.0018616351298987865, 0.007186433300375938,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>UNITED STATES SECURITIES AND EXCHANGE COMMISSI...</td>\n",
       "      <td>Nasdaq Stock Market LLC Nasdaq Global Select M...</td>\n",
       "      <td>answer not available in context</td>\n",
       "      <td>Dates not available.</td>\n",
       "      <td>[0.009593687951564789, 0.006503074895590544, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>UNITED STATES SECURITIES AND EXCHANGE COMMISSI...</td>\n",
       "      <td>registrant was required to submit such files Y...</td>\n",
       "      <td>answer not available in context</td>\n",
       "      <td>dates not available</td>\n",
       "      <td>[0.02656714990735054, -0.007002761587500572, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Large accelerated filer Accelerated filer Non ...</td>\n",
       "      <td>Large accelerated filer Accelerated filer Non ...</td>\n",
       "      <td>answer not available in context</td>\n",
       "      <td>June 30 2020: aggregate market value of shares...</td>\n",
       "      <td>[0.0011118471156805754, -0.0008834037580527365...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20210203_alphabet_10K.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Large accelerated filer Accelerated filer Non ...</td>\n",
       "      <td>2020 was approximately 849 7 billion For purpo...</td>\n",
       "      <td>answer not available in context</td>\n",
       "      <td>The date 2020 is mentioned in the context.</td>\n",
       "      <td>[-0.014244736172258854, -0.005289070773869753,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   file_name file_type  page_number  \\\n",
       "0  20210203_alphabet_10K.pdf       pdf          1.0   \n",
       "1  20210203_alphabet_10K.pdf       pdf          1.0   \n",
       "2  20210203_alphabet_10K.pdf       pdf          1.0   \n",
       "3  20210203_alphabet_10K.pdf       pdf          2.0   \n",
       "4  20210203_alphabet_10K.pdf       pdf          2.0   \n",
       "\n",
       "                                             content  \\\n",
       "0  UNITED STATES SECURITIES AND EXCHANGE COMMISSI...   \n",
       "1  UNITED STATES SECURITIES AND EXCHANGE COMMISSI...   \n",
       "2  UNITED STATES SECURITIES AND EXCHANGE COMMISSI...   \n",
       "3  Large accelerated filer Accelerated filer Non ...   \n",
       "4  Large accelerated filer Accelerated filer Non ...   \n",
       "\n",
       "                                              chunks  \\\n",
       "0  UNITED STATES SECURITIES AND EXCHANGE COMMISSI...   \n",
       "1  Nasdaq Stock Market LLC Nasdaq Global Select M...   \n",
       "2  registrant was required to submit such files Y...   \n",
       "3  Large accelerated filer Accelerated filer Non ...   \n",
       "4  2020 was approximately 849 7 billion For purpo...   \n",
       "\n",
       "                                   predicted_answer  \\\n",
       "0  1600 Amphitheatre Parkway Mountain View CA 94043   \n",
       "1                   answer not available in context   \n",
       "2                   answer not available in context   \n",
       "3                   answer not available in context   \n",
       "4                   answer not available in context   \n",
       "\n",
       "                                     predicted_dates  \\\n",
       "0     The fiscal year end date is December 31, 2020.   \n",
       "1                               Dates not available.   \n",
       "2                                dates not available   \n",
       "3  June 30 2020: aggregate market value of shares...   \n",
       "4         The date 2020 is mentioned in the context.   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.0018616351298987865, 0.007186433300375938,...  \n",
       "1  [0.009593687951564789, 0.006503074895590544, -...  \n",
       "2  [0.02656714990735054, -0.007002761587500572, -...  \n",
       "3  [0.0011118471156805754, -0.0008834037580527365...  \n",
       "4  [-0.014244736172258854, -0.005289070773869753,...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_data_sample_head['embedding'] = pdf_data_sample_head['chunks'].apply(lambda x: embedding_model_with_backoff([x]))\n",
    "pdf_data_sample_head[\"embedding\"] = pdf_data_sample_head.embedding.apply(np.array)\n",
    "pdf_data_sample_head.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the heart of this method. First, you can define a function `get_context_from_question`, which takes the:\n",
    "* `valid_question` user wants to ask, \n",
    "* 'vector_store`: vector db store, which we created in the last step and,\n",
    "* `sort_index_value`: The value defines how many chunks will be picked after running the sort on the cosine similarity score. \n",
    "\n",
    "The function will take the `valid_question`, create the embeddings, and do the dot product (cosine similarity) with all the chunks you passed in the vector store. Once you have the score, you can sort the results in decreasing order and pick chunks per the `sort_index_value` value as a combined string. \n",
    "\n",
    "This will become your context for the question asked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dot_product(row):\n",
    "    return np.dot(row,query_vector)\n",
    "\n",
    "def get_context_from_question(valid_question, \n",
    "                              vector_store,\n",
    "                              sort_index_value=2):\n",
    "    global query_vector\n",
    "    query_vector = np.array(embedding_model_with_backoff([valid_question]))\n",
    "    top_matched = vector_store[\"embedding\"].apply(get_dot_product).sort_values(ascending=False)[:sort_index_value].index\n",
    "    context = \" \".join(vector_store[vector_store.index.isin(top_matched)]['chunks'].values)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a general function that always gets you custom relevant context for the question, you can call it with every new question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaLM Predicted: 1600 Amphitheatre Parkway Mountain View CA 94043\n",
      "CPU times: user 9.53 ms, sys: 2.63 ms, total: 12.2 ms\n",
      "Wall time: 970 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "#your question for the documents \n",
    "valid_question = \"what is the address for google headquarter\"\n",
    "\n",
    "#get the custom relevant chunks from all the chunks in vector store. \n",
    "context = get_context_from_question(valid_question, \n",
    "                                    vector_store=pdf_data_sample_head,\n",
    "                                    sort_index_value=1 #Top N results to pick from embedding vector search\n",
    "                                   )\n",
    "\n",
    "#Prompt for Q&A which takes the custom context found in last step. \n",
    "prompt = f\"\"\" Answer the question as precise as possible using the provided context. If the answer is\n",
    "            not contained in the context, say \"answer not available in context\" \\n\\n\n",
    "            Context: \\n {context}?\\n\n",
    "            Question: \\n {valid_question} \\n\n",
    "            Answer:\n",
    "          \"\"\"\n",
    "\n",
    "#Call the PaLM API on the prompt. \n",
    "print(\"PaLM Predicted:\",text_generation_model_with_backoff(\n",
    "    prompt=prompt\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the best part of this method is that you don't have to call the API multiple times. Instead, just one time, and it figured out the answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaLM Predicted: Larry and Sergey\n",
      "CPU times: user 0 ns, sys: 10.7 ms, total: 10.7 ms\n",
      "Wall time: 1.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "valid_question = \"who are the foudners of google?\"\n",
    "\n",
    "context = get_context_from_question(valid_question, \n",
    "                                    vector_store=pdf_data_sample_head,\n",
    "                                    sort_index_value=1 #Top N results to pick from embedding vector search\n",
    "                                   )\n",
    "\n",
    "prompt = f\"\"\" Answer the question as precise as possible using the provided context. If the answer is\n",
    "            not contained in the context, say \"answer not available in context\" \\n\\n\n",
    "            Context: \\n {context}?\\n\n",
    "            Question: \\n {valid_question} \\n\n",
    "            Answer:\n",
    "          \"\"\"\n",
    "print(\"PaLM Predicted:\",text_generation_model_with_backoff(\n",
    "    prompt=prompt\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look into this method's various pros and cons to summarize what you have done. \n",
    "\n",
    "**Pros:**\n",
    "\n",
    "*  Fast: this is fast since it doesn't require the API to be executed on all the chunks.\n",
    "\n",
    "**Cons:**\n",
    "*  The vector document store can run into a vast length, and cosine similarity and basic mathematics can become slow. \n",
    "\n",
    "In the following coming notebooks in this journey, you will see the operationalization of the foundation methods using GCP and open-source products. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "question_answering.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
